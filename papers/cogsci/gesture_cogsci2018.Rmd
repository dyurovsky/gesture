---
title: "Children gesture when speech is slow to come"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Daniel Yurovksy, Madeline Meyers, Nicole Burke, Amanda Woodward, 
         \and Susan Goldin-Meadow \\
         \texttt{\{yurovsky, mcmeyers, nicoleburke, woodward, sgm\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}


abstract: 
    "We test this prediction in a corpus of videos of parent-child interaction in the home recorded longitudinally from 14- to 34- months. naturalistic parent-child o"
    
keywords:
    "communication; language acquisition; gesture; corpus analysis"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", 
                      fig.path='figs/', echo = F, warning = F, cache = F, 
                      message = F, sanitize = T)
```

```{r, libraries}
library(tidyverse)
library(png)
library(grid)
library(xtable)
library(feather)
library(tidyverse)
library(irr)
library(tidyboot)
library(directlabels)
library(lme4)
library(broom)

theme_set(theme_classic(base_size = 10))
```

# Introduction

Children learn a striking amount of language in their first few years of life--thousands of sounds, words, grammatical categories, and the combinatoric properties that allow them to be combined to produce meaningful utterances [@clark2009]. They also come to understand what all of this language *is for*: communicating with other people [@zipf1949]. Further, there is good reason to think that these two problems are deeply interwined. The language that children hear is rarely a running commentary on the world around them--when a child's parents return home work, they are much more likely to say "whatcha been doing all day?" than "I am opening the door" [@gleitman1990]. Understanding that their parent is not trying to tell them about the door may go a long a way to learning what the words they are hearing mean.

The understanding that speakers productions are intended to communicate information is at the core of the kinds of inferences that adults routinely make when processing language. These pragmatic inferences, for instance, are the reason that hearing a speaker say that they ate "some of the cookies," causes us to think that some cookies still remain on the plate [@grice1969]. Children's ability to perform these kinds of complex inferences appears relatively late in language development [@noveck2001]. However, a growing body of empirical evidence shows that a basic undertanding of the communicative purpose of language is already present in the first year of life [@tomasello2000]. For instance, children appear to understand that speakers communicate information to other adults, even if they themselves do not understand the words being said [@vouloumanos2012; @vouloumanos2014]. But is this understanding of communicative goals present in children's *language production* as well?

The core of extended communicative interactions is taking turns: participants each contribute to the discourse, but only one at a time [@sacks1974]. Turn taking is not only universal among both modern and indigenous cultures, the length of time between turns is highly stereotyped, and predicted by the same factors across cultures [@stivers2009]. Evidence from both early observational studies and more recent experiments suggests that tracking of turn boundaries emerges early in infancy--perhaps in the course of scripted interactions like patty cake [@bruner1983; @casillas2017].

The regularity of these turns makes communication inherently time constrained: If you stop talking for too long, you lose your turn. Adults are sensitive to this time pressure, for instance producing filled pauses like "um" when they are having difficulty retrieving the words they want to produce in order to signal their desire to hold onto their turn [@clark2002]. If retrieval is still unsuccessful, lingusitically-proficient adults can opt for an alternative word or even a description that gives their interlocutor enough information to help retrieve the word for them [@clark1989]. Children still learning their native language, for whom such strategies are unavailable, might resort to an alternative mode of communcation: pointing. 

Children produce deictic gestures early in infancy, and appear to understand that these gestures both direct attention and communicate intentions by the time they are 12-months-old [@liszkowski2007; @tomasello2007]. Around the same time, infants begin producing their first spoken words [@bloom2000]. Over the next few years, infants will produce many more words, and need to rely less on deictic gesture to communicate. However, while children master some words early, others which are less frequent may remain difficult to retrieve and produce. If children, like adults, are sensitive to the time pressures of communication, then then they may use gesture even for *known* words if these words are slow to come.

## Communication as a race between modalities

When children wish to share their interest in an object with a caregiver, they have two modalities available to them. One possibility is to use spoken language, producing the canonical label for it (e.g. "ball"). Alternatively, they can use a deictic gesture, e.g. a point, to draw the caregiver's attention to it. When should children use each of these modalities? 

If the child does not know that the object is called "ball," they have no choice but to point. However, if they do know its label, time pressure on communication produces a race between modalities. If the child can recall the word quickly, they should prefer to use language--as speech is less effortful than pointing [@zipf1949]. However, if the recall process of recalling and producing the word is progressing too slowly, the child risks losing their conversational turn and should instead point. 

```{r lba-mod, fig.env = "figure", fig.align='center', set.cap.width=T, num.cols.cap=1, fig.cap = "Referential communication as a race between speech and gesture. The drift rate of pointing should be independent of referent, but speech should vary with properties of words, e.g. frequency"}
img <- png::readPNG("figs/lba_mod_simple.png")
grid::grid.raster(img)
```

This kind of race model can be formalized nicely as two competing accumulators [see e.g. @brown2008]. Each modality accumulates activation at its own independent rate, and whichever is the first to reach threshold wins the race and is chosen as the referential modality (Figure \ref{fig:lba-mod}). Although the difficulty of pointing may vary due to issues of proximity of the speakers to each-other, the location of the target referent, etc., the difficulty of pointing should in general be independent of the thing being pointed to. On the other hand, the difficulty of recalling and producing a word varies from word to word. In adults, this difficulty is influenced by many features of the word, including the phonology and orthography of both the word and its neighbors in the lexicon [see e.g. @vitevitch2008]. Here we focus on just one--contributor: Input frequency [@wingfield1968]. The more frequently we hear a word, the easier it is for us to retrieve and produce it. Children's *language processing* shows similar effects of frequency--children's speed and accuracy of known words increases as they become more frequent [@swingley1999]. If their *language production* is similarly affected by frequency, than the rate of the speech accumulator should increase as frequency increases, resulting in it winning the race for reference more often.

The utility of this framework is that it makes detailed predictions about the relationship between modality and production time as features of the target referent change. We test three specific predictions of this model in children's spontaneous productions from 14- to 34-months:

1. As the frequency of a referent in children's input increases, they should be relatively more likely to use speech, and less likely to use gesture to communicate about it.

2. As children develop and learn more language, words should be known better and thus be easier to retrieve. Thus, speech should win the race more often--especially for low frequency words.

3. Recent use of a word should make it easier to retrieve, thus children should be relatively more likely to use speech for low frequency referents in if they have occurred previoulsy in the same discourse than at baseline.

# Method

```{r load_corpus, cache = T}

referents <- read_feather("../../feathers/referents.feather") %>%
  mutate(freq_cut = 5- as.numeric(cut(log(freq), 4)))

coded_responses <- read_feather("../../feathers/coded_responses.feather") %>%
  select(-freq_cut, -rank, - freq) %>%
  left_join(referents)

demos <- read_feather("../../feathers/demos.feather") %>%
  filter(!id %in% c(47, 110)) %>%
  mutate(income_factor = factor(round(income), levels = c(1, 2, 3, 4, 5, 6), 
                       labels=c("Less than $15,000", "$15,000 to $34,999",
                                "$35,000 to $49,999", "$50,000 to $74,999",
                                "$75,000 to $99,999", 
                                "greater than $100,000")))
         
subjs <- coded_responses %>%
  distinct(subj) %>%
  pull()

ages <- coded_responses %>%
  distinct(age) %>%
  pull()
# 
# income_labs<-c("Less than $15,000", "$15,000 to $34,999", "$35,000 to $49,999", "$50,000 to $74,999", "$75,000 to $99,999", "Greater than $100,000")
```

The data analyzed here are transcriptions of recordings parent-child interactions in the homes of `r length(subjs)` children from the Chicagoland area. Each recording was ~90min long, and participants were given no instructions about how to interact--the goal was to observe the natural ecology of language learning. Each child was recorded `r length(ages)` times at 4-month intervals starting at `r min(ages)`-mo. and ending at `r max(ages)`-mo. 

## Participants

These children's data was drawn from the larger Language Development Project dataset pseudo-randomly to preserve the socio-economic, racial, and gender diversity representative of the broader Chicago community. Of the `r length(subjs)` children, `r demos %>% summarise(female = sum(sex == "F"))` were girls, `r demos %>% summarise(black = sum(race == "BL"))` were Black and `r demos %>% summarise(mixed = sum(race == "2+"))` were Mixed-Race. Familes spanned a broad range of incomes, with `r demos %>% filter(income == min(income)) %>% nrow()` families earning `r demos %>% filter(income == min(income)) %>% pull(income_factor) %>% first()` and `r demos %>% filter(income == max(income)) %>% nrow()` family earning `r demos %>% filter(income == max(income)) %>% pull(income_factor) %>% first()`. The median family income was `r demos %>% filter(income == median(income)) %>% pull(income_factor) %>% first()`. 

## Data Processing

The original Language Development Project transcripts consist of utterance-by-utterance transcriptions of the 90 minute recordings in CHAT format [@macwhinney2000], as well as a transcription of all communicative gestures produced by children and their conversational partners, including conventional gestures (e.g. waving "bye"), representational gestures (e.g. tracing the shape of a square), and deictic gestures (e.g. pointing to a ball).

```{r coding-table, results="asis", tab.env = "table", cache = T}
coding_table <- read_csv("figs/coding_table.csv") %>%
  remove_rownames() %>%
  select(-referent) %>%
  xtable(caption = "An example of the output of data processing",
         align = "llp{2.5cm}lll",
         label = "tab:coding-table")

print(coding_table, type = "latex", comment = F, table.placement = "tb",
      include.rownames = FALSE,
      size="\\fontsize{8pt}{8pt}\\selectfont")
```


For each of these communicative acts, we coded all concrete noun referents indicated in either the spoken or gestural modality (see Table \ref{tab:coding-table}). As it is difficult both to gesture about, and to code, gestures for abstract entities like “weekend,” we focused only on nouns that could be referred to in either gesture or speech. Spoken referents were coded only if a noun label was used (e.g. no pronouns were included), and only deictic gestures were counted as referential to minimize ambiguity in coding. Synonyms, nicknames, and proper nouns were all coded according to a manual that can be found in the linked github repostory.

## Reliability

In order to ensure the integrity of the coded data for further analyses, we first assessed inter-rater reliability, and then assessed whether the coded referents were present in the scene.

### Inter-Rater Reliability

```{r reliability, cache = T}
nicole_responses <- 
  read_feather("../../feathers/interrater_data_nicole.feather") %>%
  select(subj, age, time, referent, modality) %>%
  mutate(coder = "Nicole") %>%
  mutate(referent = if_else(is.na(referent), "NA", referent))

maddie_responses <- 
read_feather("../../feathers/interrater_data_maddie.feather") %>%
  select(subj, age, time, referent, modality) %>%
  mutate(coder = "Maddie") %>%
  mutate(referent = if_else(is.na(referent), "NA", referent))

irr_data <- full_join(nicole_responses, maddie_responses, 
                      by = c("time", "subj", "age", "modality")) %>%
  filter(!is.na(modality))

kappa <- irr_data %>%
  select(referent.x, referent.y) %>%
  kappa2()

```

To assess the reliability of referent coding, 25% of the transcripts were coded by a second indepdenent coder. Reliability between coders was good (Cohen's $\kappa$ = `r round(kappa$value,2)`). Issues and discrepancies in coding decisions were dicussed and resolved during the formation of a coding manual.  

### Presence of referents

```{r presentness_data, cache = T}
reliability_data<-read_feather("../../feathers/pres_reliability.feather")
 
child_reliability <- reliability_data %>%
   filter(person == "child") %>%
   select(ref_pres, ref_predicted) %>%
   kappa2()
 
all_reliability<-reliability_data %>%
   select(ref_pres, ref_predicted) %>%
   kappa2()

presentness <- coded_responses %>%
  mutate(ref_predicted = as.numeric(ref_predicted))

```

Although we coded for concrete referents that had the potential to be produced either in speech or in gesture, we found that these referents were not always physically present in the environment. If a referent was not present, it could not be referred to in the gestural modality--potentially biasing our analyses. After coding all referents from the transcripts, the primary coder judged whether each was likely to be present in the scene according to a list of criteria described in the coding manual. Across the 60 transcripts, `r presentness %>% summarise(present = mean(ref_predicted)) %>% pull() %>% round(2) * 100`% of referents were judged to be present. Absent referents were included in estimates of input frequency, but excluded from analysis of production modality.

Reliability for referent presentness was caluclated for 5% of the data by comparison of judgments created by the primary coder and observations of video data. The reliability was acceptable for child-produced referents (Cohen's $\kappa$= `r round(child_reliability$value,2)`), as well as for all referents in the dataset (Cohen's $\kappa$=`r round(all_reliability$value,2)`). 

# Results

The key predictions of our race model of reference all connect the ease of recall of a referent's spoken label. Although ease of recall is likely related to a number of factors (e.g. phonotactic probability, neighborhood density, etc), we focus here on one easily quantifiable and well-attested predictor: input frequency [@wingfield1968]. 

## Estimating Frequency

To estimate the input frequency of each referent in the corpus, we summed its frequency of use across all children and parents and across both the speech and gestural modalities. This estimator is of course imperfect-- It assumes, for instance, that every child receives the same input, and that input frequency is stationary across development. Nonetheless, because of the difficulty of estimating these frequencies well, especially from a corpus of this size, we felt that a more complex estimator would introduce more error than it was worth.

```{r freq-fig,  fig.width=3.5, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "Referents varied widely in their frequency of use, appearing approximately Zipfian. We predict that referents frequent in the input--like baby--should be be more likely to emerge in speech than infrequent referents like cauliflower", cache = T}
label_refs <- referents %>%
  filter(referent %in% c("mom", "baby", "train", "dinosaur", "zucchini",
                          "cauliflower", "windchime"))

ggplot(referents, aes(x = rank, y = freq)) + 
  geom_point(size = .1) + 
  scale_x_log10(name = "Rank Frequency", breaks = c(1, 10, 100, 1000),
                limits = c(.95,1800)) + 
  scale_y_log10(name = "Frequency", breaks = c(1, 10, 100, 1000)) +
  geom_label(data = label_refs, aes(label = referent), color = "#e41a1c",
             size = 2) 

```

Figure \ref{fig:freq-fig} shows the frequency distribution of the `r nrow(referents)` individual referents in this corpus across all recordings. As with many other frequency distributions in language, referential frequencies were approximately Zipfian, appearing approximately linear on a log-log scale [@piantadosi2014]. These frequency estimates were used to test the first key prediction of the race model of communcation: Labels for referents that are easier to retrieve from memory are more likely to be produced in speech.

## Predictions 1 and 2: The effects of frequency

```{r tradeoff_estimate, cache = T}
model_data <-  coded_responses %>%
  filter(ref_predicted == "1", person == "child")

plotting_data <- model_data %>%
  group_by(person, age, subj, freq_cut, modality) %>%
  summarise(n = n()) %>%
  mutate(prob = n/sum(n)) %>%
  group_by(person, age, modality, freq_cut) %>%
  tidyboot_mean(prob, na.rm = T) %>%
  ungroup() %>% 
  filter(modality != "both") %>%
  mutate(age = paste0(age, " months")) %>%
  complete(age, modality, person, freq_cut, 
           fill = list(person = "child", mean = 0, ci_upper = 0, ci_lower = 0))
```

```{r tradeoff-plot, fig.env = "figure*", fig.pos = "h", fig.width=5.5, fig.height=3.5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Speech-gesture tradeoff"}
ggplot(plotting_data, aes(x = freq_cut, y = mean, color = modality,
                          label = modality)) +
  geom_line()+
  facet_wrap( ~ age) + 
  geom_pointrange(aes(ymax = ci_upper, ymin = ci_lower), 
                 position = position_dodge(.5)) + 
  scale_color_brewer(palette = "Set1") +
  geom_dl(method = list(dl.trans(x=x +.2), "last.qp", cex=.7)) +
  scale_x_continuous(limits = c(.5, 5.7),
                     breaks = seq(1, 4),
                     name = "Frequency Quartile") +
  ylab("Production Probability") + 
  theme(legend.position = "none")

```

```{r tradeoff_model, cache = T}
tidy_table <- function(mermod) {
  tidy(mermod) %>%
    filter(group == "fixed") %>%
    select(-group) %>%
    mutate(p.value = if_else(p.value < .001, "<.001", 
                             as.character(p.value))) %>%
    mutate(estimate = round(estimate, digits = 2),
           std.error = round(std.error, digits = 2),
           estimate = paste0(estimate, " (", std.error, ")"),
           estimate = sub("0.", ".", estimate)) %>%
    select(-std.error) %>%
    rename(`Z-value` = statistic,
           `p-value` = p.value)

}


tradeoff_model <- glmer(cbind(modality != "gesture", modality== "gesture") ~ 
                          log(freq) * scale(age) + (scale(age)|subj),
                        family = "binomial", data = model_data)
```

```{r model-table, results="asis", tab.env = "table", cache = T}
model_table <- tidy_table(tradeoff_model) %>%
  mutate(term = c("Intercept", "log frequency", "age",
                  "log frequency * age")) %>%
    xtable(caption = "Speech gesture tradeoff lmer",
           label = "tab:model-stats")

print(model_table, type = "latex", comment = F, table.placement = "tb",
      include.rownames = FALSE)
```

```{r sub_model, cache = T, eval = F}
sub_data <- model_data %>%
  group_by(freq, subj, age, referent, modality) %>%
  summarise(n = n()) %>%
  spread(modality, n) %>%
  filter(!is.na(gesture) & (!is.na(speech) | !is.na(both))) %>%
  mutate(speech = max(speech + both, speech, both, na.rm = T)) %>%
  select(-both)

sub_model <- glmer(cbind(speech, gesture) ~ log(freq) * scale(age) +
                     (scale(age)|subj), family = "binomial", data = sub_data)

tidy_table(sub_model)
````

Figure \ref{fig:tradeoff-plot} shows the distribution of speech/gesture. Table \ref{tab:model-table} shows the results of a mixed effects model. All of the effects remained significant even after only referents used in both modalities in the same 90 minutes session were included.

## Prediction 3: Recent referents get a boost

Bag of words model, probability under poisson, etc.

# Discussion



<!-- Even before they can produce, or even maybe know, the words for many objects in the world around them, infants engage their social partners in reciprocal interactions around objects they find interesting [@bruner1983]. Even before they can walk, children will engage in object-focused bids for an adult partner's attention, either pointing to a salient object of interest or carrying it over to share in joint attention [@tomasello2007,@karasik2011]. These bids can produce many of the same kinds of linguistic responses from parents as do spoken referential acts. -->

<!-- The goal of this work is to begin building a bridge between these two different ways in which children can communicate with conversational partners; to take a step in the direction of a generative model of children's referential productions. The primary target of interest will be predicting--for a given referential event--whether children are likely to use language, or are instead more likely to point or use another deictic gesture.  -->


Young children are inundated with language, hearing on the order of 30 million words by the time they are four years old [@hart1995]. From the statistical relationships within and among these words, children must discover the latent structures that allow them to become fluent speakers of their native language. Some of these words will be overheard, addressed by one parent to another or a sibling to a friend. However, some will be directed to the child, and these child-directed words maybe be particularly supportive of learning [@weisleder2013]. Child-directed speech differs from adult-directed speech along a number of dimensionsChild-directed speech is not merely a corpus of well-formed language; It is contingent on the child's own attention, interests, and prior knowledge, and thus can be directed *by the child themself*.


Young children are notorious for asking questions "why?" In her analysis of 5 children from the Brown and Kuczaj corpora in CHILDES, @chouinard2007 reports children asking over 100 questions per hour they interaction with adults over the 2-5 year range. These questions are powerful because they allow children to learn about two important things simultaneously: The causal relationships in the world around them, and also about the structure of language itself. By driving the discourse into predictable areas of content, they can reduce referential ambiguity in learning new language for this content.

Long before they can explicitly direct their input with wh-questions, children can sometimes acchieve a similar outcome simply by referring to objects in their environment. Having observed a referential event, parents will often follow-in with expansions and additional information about the child's target of interest [@clark2014]


# Acknowledgements

This research was funded by a James S. McDonnell Foundation Scholar Award to DY and National Institutes of Health P01HD40605 to SGM.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
